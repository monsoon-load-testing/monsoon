NORMALIZER:
  - figure out when to normalize data based on timestamp
    - every 15 seconds, check if there is a file with a timestamp > current normalizedTimestamp
      - YES -> process data for the current normalizedTimestamp
      - NO -> do nothing (check again in 15 seconds)
  - normalize data
  - send data to S3
  - ignore data with timestamp > stopTime
  - after data is sent to S3, change prefix from ready to sent

Data shape:


Replaces option1 as the new data shape:
filename = 34f3n98-Load Main Page-134.json
{
    stepName: "Load Main Page",
    userId: "34f3n98",
    stepStartTime: 134,
    metrics: {
        responseTime: 900,
    }
}

The data we're outputting:
filename: Load_Main_Page-130.json
{
  user1: {
    stepName: "Load Main Page",
    userId: "34f3n98",
    stepStartTime: 134,
    metrics: {
        responseTime: 900,
    },
  },

  user2: {
    stepName: "Load Main Page",
    userId: "77hg43",
    stepStartTime: 132,
    metrics: {
        responseTime: 600,
    },
  },

  user3: {
    stepName: "Load Main Page",
    userId: "oi87cd",
    stepStartTime: 144,
    metrics: {
        responseTime: 700,
    },
  },
}

filename: Go_To_Bin-130.json


Assumption: all json files are stored in dummyResults folder (just for now)

Data normalization process:
  - from time window 130 - 144: user1 has 5 json data points, user2 has 6 json data points, user3 has 7
  - take average of 5 data points (response time) from user1, of 6 data points of user2, of 7 data points from user3
  - nest the result into one json file
  - S3 bucket prefix: Load_Main_Page-130: per step - per normalized timestamp

Algorithm (for 3 users - faster to generate dummyResults):
  - check 130 - 144: any file that has stepStartTime inside the range (inclusive) -> pull all of it (note: observer generates this file)
observer's generated json data form: 34f3n98-Load Main Page-134.json
/*
  Scenario: one user, one step
  In the time window of 130-145, how to grab all of the files from all of users, all of steps?
      - batch based on step
          -within each step batch, sort based on stepStartTime 
          [130, 145, 160, 175]
          - stepStartTime >= 130 && stepStartTime < 145


START HERE: add a way to "close off" a window in order to deal with stragglers
                 - sleep for 30 seconds before starting to make the finalized .json files

Scenario (according to dummyResults):
   1 user (one runner.js)
   2 steps (inside each loop, record two steps)
   4 results JSONs per step


POPULATING THE BUCKETS OBJ
// create an array of normalized timestamps
// we need:
     - originTimestamp
     - window (15)
     - overall test duration
     
const normalizedTimestamps = [originTimestamp, 15 + originTimestamp, 30 + originTimestamp, ....<= originTimestamp + overall test duration]
let buckets = {
  15: [],
  30: [],
  45: [],
  60: [],
};

[15, 30, 45, 60]
1) make the empty buckets obj - DONE
2) Iterate through each file in dummyResults
      -get at both filename AND contents - DONE
3) Put that .json file into the correct bucket:
      add in the stepName (grab this from the filename) - DONE
      for (let idx = 1; idx < normalizedTimestamps.length; idx++) {
        if (fileContents.stepStartTime < normalizedTimestamps[idx]) {
          let normalizedTimestamp = normalizedTimestamps[idx - 1];
          buckets[normalizedTimestamp].push(fileContents);
        } 
      }

let buckets = {
  16578756863472000: [{userId: "77hg43",
                       stepName: "Go to bin",
                       stepStartTime: 132,
                       metrics: {
                           responseTime: 600,
                       },]
}

PROCESSING BEFORE SENDING TO S3 BUCKET
iterate through normalizedTimestamps:
   - break down buckets[current normalizedTimestamp] BY STEP
   - do averaging calculation for each step
   - make a new JSON with ALL users at that stepName/normalizedTimestamp combination
   - data structure:
      - stepName-normalizedTimestamp:
      - examples:
      - LoadMainPage-15.json -> {
        34f3n98: {
          stepName: "Load Main Page",
          userId: "34f3n98",
          stepStartTime: 17,
          metrics: {
            normalizedResponseTime: 900, // calculated average responseTime
          },
        },
        ...other users...
      }
      - send to S3 bucket
/*
  - normalize the data (one batch) -> send it to S3 bucket in the prefix 34fn32-Load Main Page-130  (options: console.log, set-up manual S3 and send them, 
send to another folder)
  - next round: Load_Main_Page-145 (check form 144 - 149)
  - continue until stopTime


When to normalize?
  - stopTime (from runner - prototype)
  - originTimestamp (from runner - prototype)
  - hard-code window = 15_000 // ms

How does normalizer.js get at the stopTime variable?
  - node environmentime
  - call the normalizer inside the runner.js (export normalizer as a module)