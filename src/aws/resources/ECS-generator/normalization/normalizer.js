/*
go from batch processing to stream processing

Algorithm to prevent stragglers (datapoint that gets to normalizer.js after MORE THAN 30 seconds)
1) normalizer receives a new datapoint
2) check if that datapoint's stepStartTime + 30 sec < Date.now() -> means this datapoint is too old, is a straggler
      -> ignore it, don't let this datapoint make it into any bucket
Note: 30 sec needs to be larger than the polling window

raw timestamps:
130
133
134
132
140
139
145

bucket1 - 122.5 - 137.5 (excluded)
bucket2 - 137.5 - 152.5

input:
  - config -> originTimestamp, timeWindow, timestamps, testDuration
  - json files being generated by weather-station (./load-generation/results)

output: 
  - FINAL BUCKET OBJECT:
  - key: stepName-normalizedTimestamp
  - value is an object. Inside the object, property is unique userId with their normalized metrics
  - value: an object with 1 or more userIds as keys:
                value: { metrics: { 
                    normalizedResponseTime: 108
                }}

- track current upperBound of the current normalizedTimestamp
- as soon as we see a raw timestamp > upperbound, package up the current normalizedTimestamp's datapoints

Algorithm:
  - const initialOffset = 15s
  - polling time after initialOffset = timeWindow

  - global object: const normalizedTimestamps = config.timestamps;
  [130, 145, 160, 175]

  - set initialOffset
  - inside the while loop: (polling every 15s)
      - if (normalizedTimstamps.length === 1) {   // COME BACK TO THIS
        break
      }
      - calculate the lowerBound & upperBound of normalizedTimestamps[0]
      - constant shouldProcess = false
      - loop through each json file generated by weather-station
          - if the stepStartTime >= upperBound {
              shouldProcess = true
              break
            }
          - else continue
      - if (shouldProcess) {
        - only process all data points in normalizedTimestamps[0] (delete it after process as well)
          - only data points with stepStartTime between lowerBound & upperBound (>=lowerBound & < upperBound)
        - send only the data points from  normalizedTimestamps[0] to S3 bucket
        - reset buckets, filteredBucket, and finalBucket 
        - chop off the normalizedTimestamps[0] -> [145, 160, 175]
      - else:
        - break
      }
  - await sleep(15s) -> process the last batch

  [130, 145, 160, 175]
  intialOffset = 15, timeWindow = 15
  at t = 160s
  lowerBound = 122.5, upperBound = 137.5
  assume we have a json file that has stepStartTime >= upperBound -> shouldProcess = true -> break
  process al data points that has stepStartTIme >= lowerBound & < upperBound and also delete all those json files
  send to S3 bucket
  normalizedTimestamps = [145, 160, 175]

  after 15s
  at t = 175s
  lowerBound = 137.5, upperBOund = 152.5
  do every thing again
  edge case: if there is a file that has stepStartTime >= 122.5 and < 137.5 (first time window)
  - only data points with stepStartTime between lowerBound & upperBound (>=lowerBound & < upperBound)
  - current lowerBound = 137.5 and upperBound = 152.5
*/

const fs = require("fs");
const AWS = require("aws-sdk");
const { nanoid } = require("nanoid");
const config = JSON.parse(
  fs.readFileSync("../load-generation/petrichor/config.json", "utf-8")
);
const sleep = (ms) => {
  return new Promise((resolve) => setTimeout(resolve, ms));
};

// testing purpose
const initializeTimestamps = (timeWindow, testDuration, originTimestamp) => {
  let currentTime = originTimestamp;
  const normalizedTimestamps = [];
  const finalTimestamp = originTimestamp + testDuration;
  while (currentTime < finalTimestamp) {
    normalizedTimestamps.push(currentTime);
    currentTime += timeWindow;
  }
  normalizedTimestamps.push(finalTimestamp);
  return normalizedTimestamps;
};

const originTimestamp = config.ORIGIN_TIMESTAMP;
const timeWindow = config.TIME_WINDOW;
const testDuration = config.TEST_LENGTH;
// const normalizedTimestamps = config.timestamps; // global object
const normalizedTimestamps = initializeTimestamps(
  timeWindow,
  testDuration,
  originTimestamp
);

const initialOffset = 15_000; // should cover the case where average response time of a request is 3s
const pollingTime = 15_000; // our use case

// fs.writeFile(
//   `./log/test.json`,
//   JSON.stringify({ success: "success log" }),
//   () => {
//     console.log("hi");
//   }
// );

async function doNormalization() {
  await sleep(initialOffset);

  let count = 0;
  while (true) {
    await sleep(pollingTime); // polling
    let shouldProcess = false;
    const [lowerBound, upperBound] = [
      normalizedTimestamps[0] - timeWindow / 2,
      normalizedTimestamps[0] + timeWindow / 2,
    ];

    try {
      const filenames = await fs.promises.readdir("../load-generation/results");
      // check if there is a file existing in the next time window
      for (let filename of filenames) {
        let fileContents = await fs.promises.readFile(
          `../load-generation/results/${filename}`,
          { encoding: "utf-8" }
        );
        fileContents = JSON.parse(fileContents);
        if (fileContents.stepStartTime >= upperBound) {
          shouldProcess = true;
          break;
        }
      }

      if (shouldProcess) {
        // loop through again filenames -> fileContents
        // process data (while also deleting those files)
        // if fileContents.stepStartTime >= lowerBound && fileContents.stepStartTime < upperBound
        let buckets = {};
        let filteredBucket = {};
        let finalBucket = {};
        buckets[normalizedTimestamps[0]] = [];
        // filename = userId-stepName-stepStartTime
        for (let filename of filenames) {
          let fileContents = await fs.promises.readFile(
            `../load-generation/results/${filename}`,
            { encoding: "utf-8" }
          );
          fileContents = JSON.parse(fileContents);

          if (
            fileContents.stepStartTime >= lowerBound &&
            fileContents.stepStartTime < upperBound
          ) {
            await fs.promises.unlink(`../load-generation/results/${filename}`);
            const stepName = filename.split("-")[1];
            fileContents.stepName = stepName;
            buckets[normalizedTimestamps[0]].push(fileContents);
          }
        }
        // await fs.promises.writeFile(
        //   `./log/${normalizedTimestamps[0]}.json`,
        //   JSON.stringify({ somename: buckets[normalizedTimestamps[0]] })
        // );
        // filteredBucket -> just the normalizedTimestamps[0] batch
        buckets[normalizedTimestamps[0]].forEach((dataPoint) => {
          const key = `${dataPoint.userId}-${dataPoint.stepName}-${String(
            normalizedTimestamp[0]
          )}`;
          if (filteredBucket[key]) {
            filteredBucket[key].push(dataPoint);
          } else {
            filteredBucket[key] = [dataPoint];
          }
        });

        // // build finalBucket -> just the nromalizedTiemstamps[0] batch
        // Object.keys(filteredBucket).forEach((key) => {
        //   let sum = 0;
        //   filteredBucket[key].forEach((dataPoint) => {
        //     sum += dataPoint.metrics.responseTime;
        //   });
        //   const normalizedResponseTime = Math.round(
        //     sum / filteredBucket[key].length
        //   );
        //   const [userId, stepName, normalizedTimestamp] = key.split("-");
        //   const newKey = `${stepName}-${normalizedTimestamp}`;

        //   if (finalBucket[newKey]) {
        //     // if newKey already exists
        //     finalBucket[newKey][userId] = {
        //       metrics: {
        //         normalizedResponseTime,
        //       },
        //     };
        //   } else {
        //     // newKey doesn't exist yet
        //     finalBucket[newKey] = {
        //       [userId]: {
        //         metrics: {
        //           normalizedResponseTime,
        //         },
        //       },
        //     };
        //   }
        // });
        // send to S3 bucket
        normalizedTimestamps.shift();
      }
    } catch (err) {
      await fs.promises.writeFile(`./log/error.json`, JSON.stringify(err));
    }

    // try {
    //   filenames.forEach(async (filename) => {
    //     const stepName = filename.split("-")[1];

    // await sleep(5000); // sleep for 5s so that the intermediate bucket can extract the data
    // downsample();
    // writeToS3();
    // writeToLocal();
  }
}

doNormalization();

// // make the buckets object
// let buckets = {};
// normalizedTimestamps.forEach((normalizedTimestamp) => {
//   buckets[normalizedTimestamp] = [];
// });

// let finalBucket = {};
// const downsample = () => {
//   const filteredBucket = filterBucket(); // based on same userId - same stepName
//   buildFinalBucket(filteredBucket);
// };

// const filterBucket = () => {
//   const filteredBucket = {};
//   normalizedTimestamps.forEach((normalizedTimestamp) => {
//     if (buckets[normalizedTimestamp].length === 0) {
//       filteredBucket[normalizedTimestamp] = [];
//     }
//     buckets[normalizedTimestamp].forEach((dataPoint) => {
//       const key = `${dataPoint.userId}-${dataPoint.stepName}-${String(
//         normalizedTimestamp
//       )}`;
//       if (filteredBucket[key]) {
//         filteredBucket[key].push(dataPoint);
//       } else {
//         filteredBucket[key] = [dataPoint];
//       }
//     });
//   });
//   return filteredBucket;
// };

// const buildFinalBucket = (filteredBucket) => {
//   Object.keys(filteredBucket).forEach((key) => {
//     let sum = 0;
//     filteredBucket[key].forEach((dataPoint) => {
//       sum += dataPoint.metrics.responseTime;
//     });
//     const normalizedResponseTime = Math.round(sum / filteredBucket[key].length);
//     const [userId, stepName, normalizedTimestamp] = key.split("-");
//     const newKey = `${stepName}-${normalizedTimestamp}`;

//     if (filteredBucket[key].length === 0) {
//       // finalBucket[key] = {};
//     } else if (finalBucket[newKey]) {
//       // if newKey already exists
//       finalBucket[newKey][userId] = {
//         metrics: {
//           normalizedResponseTime,
//         },
//       };
//     } else {
//       // newKey doesn't exist yet
//       finalBucket[newKey] = {
//         [userId]: {
//           metrics: {
//             normalizedResponseTime,
//           },
//         },
//       };
//     }
//   });
//   // console.log("final bucket", finalBucket);
// };

// const writeToS3 = () => {
//   const BUCKET_NAME = "monsoon-load-testing-bucket";
//   // AWS.config.update({
//   //   accessKeyId: process.env.AWS_ACC_KEY,
//   //   secretAccessKey: process.env.AWS_SECRET_KEY,
//   // });

//   const s3 = new AWS.S3();
//   for (let filename in finalBucket) {
//     // stepName-normalizedTimestamp
//     // if there is no data in the time window -> normalizedTimestamp
//     let [stepName, normalizedTimestamp] = filename.split("-");
//     if (!normalizedTimestamp) {
//       normalizedTimestamp = "noDataPointsForThisTimeWindow(s)";
//     }
//     const params = {
//       Bucket: BUCKET_NAME,
//       Key: `${normalizedTimestamp}/${stepName}/${nanoid(7)}.json`, // File name you want to save as in S3
//       Body: JSON.stringify(finalBucket[filename]),
//     };
//     s3.upload(params, function (err, data) {
//       if (err) {
//         console.log(err);
//       }
//       // console.log(`File uploaded successfully. ${data.Location}`);
//     });
//   }
// };

// const writeToLocal = () => {
//   for (let filename in finalBucket) {
//     // stepName-normalizedTimestamp
//     // if there is no data in the time window -> normalizedTimestamp
//     let [stepName, normalizedTimestamp] = filename.split("-");
//     if (!normalizedTimestamp) {
//       normalizedTimestamp = "noDataPointsForThisTimeWindow(s)";
//     }
//     const outPutFileName = `${normalizedTimestamp}-${stepName}-${nanoid(
//       7
//     )}.json`;
//     const directoryName = "./output";
//     if (!fs.existsSync(directoryName)) {
//       fs.mkdirSync(directoryName);
//     }

//     const json = JSON.stringify(finalBucket[filename]);
//     console.log(json);
//     fs.writeFile(`${directoryName}/${outPutFileName}`, json, (err) => {
//       // if (err) throw err;
//       if (err)
//         throw new Error(
//           `${normalizedTimestamp}, ${stepName}, ${outPutFileName}`
//         );
//       console.log(`${outPutFileName} has been saved`);
//     });
//   }
//   // buckets = {};
//   // finalBucket = {};
// };

// async function doNormalization() {
//   await sleep(15_000); // initial offset

//   while (true) {
//     await sleep(15_000); // polling
//     try {
//       const filenames = await fs.promises.readdir("../load-generation/results");
//       filenames.forEach(async (filename) => {
//         const stepName = filename.split("-")[1];

//         let fileContents = await fs.promises.readFile(
//           `../load-generation/results/${filename}`,
//           {
//             encoding: "utf-8",
//           }
//         );
//         fs.unlink(`../load-generation/results/${filename}`, (err) => {
//           if (err) {
//             console.log(err);
//           }
//           // console.log("deleted: ", filename);
//         });
//         fileContents = JSON.parse(fileContents);
//         fileContents.stepName = stepName;
//         for (let idx = 0; idx < normalizedTimestamps.length; idx++) {
//           const [lowerBound, upperBound] = [
//             normalizedTimestamps[idx] - timeWindow / 2,
//             normalizedTimestamps[idx] + timeWindow / 2,
//           ];
//           if (
//             fileContents.stepStartTime >= lowerBound &&
//             fileContents.stepStartTime < upperBound
//           ) {
//             let normalizedTimestamp = normalizedTimestamps[idx];
//             buckets[normalizedTimestamp].push(fileContents);
//           }
//         }
//       });
//     } catch (err) {
//       console.error(err);
//     }

//     await sleep(5000); // sleep for 5s so that the intermediate bucket can extract the data
//     downsample();
//     writeToS3();
//     // writeToLocal();
//   }
// }

// doNormalization();
